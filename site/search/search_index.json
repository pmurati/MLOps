{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MLOps Source Code A quick overview of the source code, used in the MLOps Pipeline.","title":"Start"},{"location":"#mlops-source-code","text":"A quick overview of the source code, used in the MLOps Pipeline.","title":"MLOps Source Code"},{"location":"api/","text":"Reference Report Module for visualization of confusion matrix, that is part of the report produced by the pipeline. plot_confusion_matrix ( cm , target_names , title = 'Confusion matrix' , cmap = None , normalize = True ) Given a sklearn confusion matrix (cm), make a plot. Usage: plot_confusion_matrix(cm = cm, normalize = True, target_names = y_labels_vals, title = best_estimator_name) Citation: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html Parameters: Name Type Description Default cm np . array confusion matrix from sklearn.metrics.confusion_matrix required target_names List [ Text ] given classification classes such as [0, 1, 2] the class names, for example: ['high', 'medium', 'low'] required title Text the text to display at the top of the matrix. Defaults to 'Confusion matrix'. 'Confusion matrix' cmap matplotlib . colors . LinearSegmentedColormap the gradient of the values displayed from matplotlib.pyplot.cm see http://matplotlib.org/examples/color/colormaps_reference.html plt.get_cmap('jet') or plt.cm.Blues. Defaults to None. None normalize bool If False, plot the raw numbers. If True, plot the proportions. Defaults to True. True Returns: Type Description plt . figure plt.gfc (plt.figure): plotted confusion matrix Source code in src\\report\\visualize.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def plot_confusion_matrix ( cm : np . array , target_names : List [ Text ], title : Text = 'Confusion matrix' , cmap : matplotlib . colors . LinearSegmentedColormap = None , normalize : bool = True ) -> plt . figure : \"\"\"Given a sklearn confusion matrix (cm), make a plot. Usage: plot_confusion_matrix(cm = cm, normalize = True, target_names = y_labels_vals, title = best_estimator_name) Citation: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html Args: cm (np.array): confusion matrix from sklearn.metrics.confusion_matrix target_names (List[Text]): given classification classes such as [0, 1, 2] the class names, for example: ['high', 'medium', 'low'] title (Text, optional): the text to display at the top of the matrix. Defaults to 'Confusion matrix'. cmap (matplotlib.colors.LinearSegmentedColormap, optional): the gradient of the values displayed from matplotlib.pyplot.cm see http://matplotlib.org/examples/color/colormaps_reference.html plt.get_cmap('jet') or plt.cm.Blues. Defaults to None. normalize (bool, optional): If False, plot the raw numbers. If True, plot the proportions. Defaults to True. Returns: plt.gfc (plt.figure): plotted confusion matrix \"\"\" accuracy = np . trace ( cm ) / float ( np . sum ( cm )) misclass = 1 - accuracy if cmap is None : cmap = plt . get_cmap ( 'Blues' ) plt . figure ( figsize = ( 8 , 6 )) plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar () if target_names is not None : tick_marks = np . arange ( len ( target_names )) plt . xticks ( tick_marks , target_names , rotation = 45 ) plt . yticks ( tick_marks , target_names ) if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] thresh = cm . max () / 1.5 if normalize else cm . max () / 2 for i , j in itertools . product ( range ( cm . shape [ 0 ]), range ( cm . shape [ 1 ])): if normalize : plt . text ( j , i , \" {:0.4f} \" . format ( cm [ i , j ]), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) else : plt . text ( j , i , \" {:,} \" . format ( cm [ i , j ]), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) plt . tight_layout () plt . ylabel ( 'True label' ) plt . xlabel ( 'Predicted label \\n accuracy= {:0.4f} ; misclass= {:0.4f} ' . format ( accuracy , misclass )) return plt . gcf () Stages Data Load Module for the data loading stage. data_load ( config_path ) Load raw data. Depending on the parameter revision_on (taken from config), a specific version of data is loaded from the DVC repo. Othewise, the iris dataset is loaded from sklearn.datasets. Parameters: Name Type Description Default config_path Text path to config required Source code in src\\stages\\data_load.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @mlflow_run def data_load ( config_path : Text ) -> None : \"\"\"Load raw data. Depending on the parameter revision_on (taken from config), a specific version of data is loaded from the DVC repo. Othewise, the iris dataset is loaded from sklearn.datasets. Args: config_path (Text): path to config \"\"\" with open ( config_path ) as conf_file : config = yaml . safe_load ( conf_file ) logger = get_logger ( 'DATA_LOAD' , log_level = config [ 'base' ][ 'log_level' ]) logger . info ( 'Get dataset' ) if config [ 'data_load' ][ 'revision_on' ]: logger . info ( 'Grab dataset version {} ' . format ( config [ 'data_load' ][ 'version' ])) data_url = dvc . api . get_url ( path = config [ 'data_load' ][ 'dataset_csv' ], repo = config [ 'data_load' ][ 'repo' ], rev = config [ 'data_load' ][ 'version' ] ) dataset = pd . read_csv ( data_url , sep = ',' ) # log data url and version tag in mlflow mlflow . log_param ( 'data_url' , data_url ) mlflow . log_param ( 'version' , config [ 'data_load' ][ 'version' ]) else : data = load_iris ( as_frame = True ) dataset = data . frame dataset . rename ( columns = lambda colname : colname . strip ( ' (cm)' ) . replace ( ' ' , '_' ), inplace = True ) logger . info ( 'Save raw data' ) dataset . to_csv ( config [ 'data_load' ][ 'dataset_csv' ], index = False ) mlflow . log_param ( 'input_rows' , dataset . shape [ 0 ]) mlflow . log_param ( 'input_cols' , dataset . shape [ 1 ]) Data Split Module for the data split stage. data_split ( config_path ) Split the featurized dataset into train/test. Parameters: Name Type Description Default config_path Text path to config required Source code in src\\stages\\data_split.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @mlflow_run def data_split ( config_path : Text ) -> None : \"\"\"Split the featurized dataset into train/test. Args: config_path (Text): path to config \"\"\" with open ( config_path ) as conf_file : config = yaml . safe_load ( conf_file ) logger = get_logger ( 'DATA_SPLIT' , log_level = config [ 'base' ][ 'log_level' ]) logger . info ( 'Load features' ) dataset = pd . read_csv ( config [ 'featurize' ][ 'features_path' ]) logger . info ( 'Split features into train and test sets' ) train_dataset , test_dataset = train_test_split ( dataset , test_size = config [ 'data_split' ][ 'test_size' ], random_state = config [ 'base' ][ 'random_state' ] ) logger . info ( 'Save train and test sets' ) train_csv_path = config [ 'data_split' ][ 'trainset_path' ] test_csv_path = config [ 'data_split' ][ 'testset_path' ] train_dataset . to_csv ( train_csv_path , index = False ) test_dataset . to_csv ( test_csv_path , index = False ) mlflow . log_param ( 'test_size' , config [ 'featurize' ][ 'features_path' ]) Evaluation Module for evaluation stage of trained model. evaluate_model ( config_path ) Evaluate the trained model. Load the trained model and the test set via the path specification in the config file. Do the prediction on the test set, compute its f1 score and plot the confusion matrix. Both are saved in the reports folder. Parameters: Name Type Description Default config_path Text path to config required Source code in src\\stages\\evaluate.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 @mlflow_run def evaluate_model ( config_path : Text ) -> None : \"\"\"Evaluate the trained model. Load the trained model and the test set via the path specification in the config file. Do the prediction on the test set, compute its f1 score and plot the confusion matrix. Both are saved in the reports folder. Args: config_path (Text): path to config \"\"\" with open ( config_path ) as conf_file : config = yaml . safe_load ( conf_file ) logger = get_logger ( 'EVALUATE' , log_level = config [ 'base' ][ 'log_level' ]) logger . info ( 'Load model' ) model_path = config [ 'train' ][ 'model_path' ] model = joblib . load ( model_path ) logger . info ( 'Load test dataset' ) test_df = pd . read_csv ( config [ 'data_split' ][ 'testset_path' ]) logger . info ( 'Evaluate (build report)' ) target_column = config [ 'featurize' ][ 'target_column' ] y_test = test_df . loc [:, target_column ] . values X_test = test_df . drop ( target_column , axis = 1 ) . values prediction = model . predict ( X_test ) f1 = f1_score ( y_true = y_test , y_pred = prediction , average = 'macro' ) cm = confusion_matrix ( prediction , y_test ) report = { 'f1' : f1 , 'cm' : cm , 'actual' : y_test , 'predicted' : prediction } logger . info ( 'Save metrics' ) # save f1 metrics file reports_folder = Path ( config [ 'evaluate' ][ 'reports_dir' ]) metrics_path = reports_folder / config [ 'evaluate' ][ 'metrics_file' ] json . dump ( obj = { 'f1_score' : report [ 'f1' ]}, fp = open ( metrics_path , 'w' ) ) logger . info ( f 'F1 metrics file saved to : { metrics_path } ' ) logger . info ( 'Save confusion matrix' ) # save confusion_matrix.png plt = plot_confusion_matrix ( cm = report [ 'cm' ], target_names = load_iris ( as_frame = True ) . target_names . tolist (), normalize = False ) confusion_matrix_png_path = reports_folder / \\ config [ 'evaluate' ][ 'confusion_matrix_image' ] plt . savefig ( confusion_matrix_png_path ) logger . info ( f 'Confusion matrix saved to : { confusion_matrix_png_path } ' ) mlflow . log_metric ( 'f1_test' , report [ 'f1' ]) Featurize Module for featurization stage of raw data sets. featurize ( config_path ) Create new features and save the processed dataset. Parameters: Name Type Description Default config_path Text path to config required Source code in src\\stages\\featurize.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @mlflow_run def featurize ( config_path : Text ) -> None : \"\"\"Create new features and save the processed dataset. Args: config_path (Text): path to config \"\"\" with open ( config_path ) as conf_file : config = yaml . safe_load ( conf_file ) logger = get_logger ( 'FEATURIZE' , log_level = config [ 'base' ][ 'log_level' ]) logger . info ( 'Load raw data' ) dataset = pd . read_csv ( config [ 'data_load' ][ 'dataset_csv' ]) logger . info ( 'Extract features' ) dataset [ 'sepal_length_to_sepal_width' ] = dataset [ 'sepal_length' ] / \\ dataset [ 'sepal_width' ] dataset [ 'petal_length_to_petal_width' ] = dataset [ 'petal_length' ] / \\ dataset [ 'petal_width' ] featured_dataset = dataset [[ 'sepal_length' , 'sepal_width' , 'petal_length' , 'petal_width' , 'sepal_length_to_sepal_width' , 'petal_length_to_petal_width' , 'target' ]] logger . info ( 'Save features' ) features_path = config [ 'featurize' ][ 'features_path' ] featured_dataset . to_csv ( features_path , index = False ) mlflow . log_param ( 'features' , featured_dataset . columns ) Train Module for the training stage. train_model ( config_path ) Train model. Train the model on the train set. Get the path to the data and the parametrization for the model used from the config. This stage uses a helper function that does a GridSearchCV, with the intend of further modularization and higher flexibility of this script. The best model is saved to the models directory as a joblib file. In addition, the model is logged via mlflow. Parameters: Name Type Description Default config_path Text path to config required Source code in src\\stages\\train.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 @mlflow_run def train_model ( config_path : Text ) -> None : \"\"\"Train model. Train the model on the train set. Get the path to the data and the parametrization for the model used from the config. This stage uses a helper function that does a GridSearchCV, with the intend of further modularization and higher flexibility of this script. The best model is saved to the models directory as a joblib file. In addition, the model is logged via mlflow. Args: config_path (Text): path to config \"\"\" with open ( config_path ) as conf_file : config = yaml . safe_load ( conf_file ) logger = get_logger ( 'TRAIN' , log_level = config [ 'base' ][ 'log_level' ]) logger . info ( 'Get estimator name' ) estimator_name = config [ 'train' ][ 'estimator_name' ] logger . info ( f 'Estimator: { estimator_name } ' ) logger . info ( 'Load train dataset' ) train_df = pd . read_csv ( config [ 'data_split' ][ 'trainset_path' ]) logger . info ( 'Train model' ) model = train ( df = train_df , target_column = config [ 'featurize' ][ 'target_column' ], estimator_name = estimator_name , param_grid = config [ 'train' ][ 'estimators' ][ estimator_name ][ 'param_grid' ], cv = config [ 'train' ][ 'cv' ] ) logger . info ( f 'Best score: { model . best_score_ } ' ) logger . info ( 'Save model' ) models_path = config [ 'train' ][ 'model_path' ] joblib . dump ( model , models_path ) mlflow . log_param ( 'chosen_estimator' , model . best_estimator_ ) mlflow . log_param ( 'parametrization' , model . best_params_ ) mlflow . log_metric ( 'F1' , model . best_score_ ) mlflow . sklearn . log_model ( model , 'model' ) Train Submodule Contains the submodule for training the model. Consider adding more options. get_supported_estimator () Return a list of supported classifiers. Returns: Name Type Description Dict Dict supported classifiers Source code in src\\train\\train.py 20 21 22 23 24 25 26 27 28 29 30 def get_supported_estimator () -> Dict : \"\"\"Return a list of supported classifiers. Returns: Dict: supported classifiers \"\"\" return { 'logreg' : LogisticRegression , 'svm' : SVC , 'knn' : KNeighborsClassifier } train ( df , target_column , estimator_name , param_grid , cv ) Train model via GridSearchCV. First, check if the estimator handed to this function is supported by the GridSearchCV subroutine. If so, do the gridsearch and return the fitted object. Parameters: Name Type Description Default df pd . DataFrame dataset required target_column Text target column name required estimator_name Text estimator name required param_grid Dict grid parameters required cv int cross-validation value required Raises: Type Description UnsupportedClassifier Raise an error if estimator is not supported by this train routine. Returns: Name Type Description clf sklearn . grid_search trained model Source code in src\\train\\train.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def train ( df : pd . DataFrame , target_column : Text , estimator_name : Text , param_grid : Dict , cv : int ): \"\"\"Train model via GridSearchCV. First, check if the estimator handed to this function is supported by the GridSearchCV subroutine. If so, do the gridsearch and return the fitted object. Args: df (pd.DataFrame): dataset target_column (Text): target column name estimator_name (Text): estimator name param_grid (Dict): grid parameters cv (int): cross-validation value Raises: UnsupportedClassifier: Raise an error if estimator is not supported by this train routine. Returns: clf (sklearn.grid_search): trained model \"\"\" estimators = get_supported_estimator () if estimator_name not in estimators . keys (): raise UnsupportedClassifier ( estimator_name ) estimator = estimators [ estimator_name ]() f1_scorer = make_scorer ( f1_score , average = 'weighted' ) clf = GridSearchCV ( estimator = estimator , param_grid = param_grid , cv = cv , verbose = 1 , scoring = f1_scorer ) # Get X and Y y_train = df . loc [:, target_column ] . values . astype ( 'int32' ) X_train = df . drop ( target_column , axis = 1 ) . values . astype ( 'float32' ) clf . fit ( X_train , y_train ) return clf Utils Logging Provides functions to create loggers. get_console_handler () Get console handler. Returns: Type Description logging . StreamHandler logging.StreamHandler: log into stdout Source code in src\\utils\\logs.py 7 8 9 10 11 12 13 14 15 16 17 18 def get_console_handler () -> logging . StreamHandler : \"\"\"Get console handler. Returns: logging.StreamHandler: log into stdout \"\"\" console_handler = logging . StreamHandler ( sys . stdout ) formatter = logging . Formatter ( \" %(asctime)s \u2014 %(name)s \u2014 %(levelname)s \u2014 %(message)s \" ) console_handler . setFormatter ( formatter ) return console_handler get_logger ( name = __name__ , log_level = logging . DEBUG ) Get logger. Parameters: Name Type Description Default name Text logger name. Defaults to name . __name__ log_level Union [ Text , int ] logging level; can be string name or integer value. Defaults to logging.DEBUG. logging.DEBUG Returns: Type Description logging . Logger logging.Logger: logger instance Source code in src\\utils\\logs.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def get_logger ( name : Text = __name__ , log_level : Union [ Text , int ] = logging . DEBUG ) -> logging . Logger : \"\"\"Get logger. Args: name (Text, optional): logger name. Defaults to __name__. log_level (Union[Text, int], optional): logging level; can be string name or integer value. Defaults to logging.DEBUG. Returns: logging.Logger: logger instance \"\"\" logger = logging . getLogger ( name ) logger . setLevel ( log_level ) # Prevent duplicate outputs in Jypyter Notebook if logger . hasHandlers (): logger . handlers . clear () logger . addHandler ( get_console_handler ()) logger . propagate = False return logger MLFlow Decorator Module for creating the MLFlow nested run decorator. mlflow_run ( wrapped_function ) Decorator to turn every stage into MLflow nested run. Parameters: Name Type Description Default wrapped_function Callable the respective function in each stage required Returns: Type Description Callable wrapped function (Callable): the input function, tracked under the respective MLFLOW_RUN_ID Source code in src\\utils\\mlflow_run_decorator.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def mlflow_run ( wrapped_function : Callable ) -> Callable : \"\"\"Decorator to turn every stage into MLflow nested run. Args: wrapped_function (Callable): the respective function in each stage Returns: wrapped function (Callable): the input function, tracked under the respective MLFLOW_RUN_ID \"\"\" config_path = 'params.yaml' with open ( config_path ) as conf_file : config = yaml . safe_load ( conf_file ) @wraps ( wrapped_function ) def wrapper ( * args , ** kwargs ): mlflow . set_experiment ( config [ 'base' ][ 'project_experiment_name' ]) # recover parent run thanks to MLFLOW_RUN_ID env variable with mlflow . start_run (): # start child run with mlflow . start_run ( run_name = wrapped_function . __name__ , nested = True ): return wrapped_function ( * args , ** kwargs ) return wrapper Initializing the nested MLFlow run Module for initializing nested MLFlow run. start_pipeline ( config_path , run_name ) Start a new MLflow run each time we launch the DVC pipeline. Print the MLFLOW_RUN_ID and save as an environment variable. This allows the possibility to track nested runs for each stage of the pipeline. Parameters: Name Type Description Default config_path Text path to config required run_name str specify run name required Source code in src\\utils\\start_pipeline.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def start_pipeline ( config_path : Text , run_name : str ) -> None : \"\"\"Start a new MLflow run each time we launch the DVC pipeline. Print the MLFLOW_RUN_ID and save as an environment variable. This allows the possibility to track nested runs for each stage of the pipeline. Args: config_path (Text): path to config run_name (str): specify run name \"\"\" with open ( config_path ) as conf_file : config = yaml . safe_load ( conf_file ) mlflow . set_experiment ( config [ 'base' ][ 'project_experiment_name' ]) with mlflow . start_run ( run_name = run_name ): print ( mlflow . active_run () . info . run_id ) mlflow . log_artifact ( 'dvc.yaml' ) mlflow . log_artifact ( config_path )","title":"API"},{"location":"api/#reference","text":"","title":"Reference"},{"location":"api/#report","text":"Module for visualization of confusion matrix, that is part of the report produced by the pipeline.","title":"Report"},{"location":"api/#src.report.visualize.plot_confusion_matrix","text":"Given a sklearn confusion matrix (cm), make a plot. Usage: plot_confusion_matrix(cm = cm, normalize = True, target_names = y_labels_vals, title = best_estimator_name) Citation: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html Parameters: Name Type Description Default cm np . array confusion matrix from sklearn.metrics.confusion_matrix required target_names List [ Text ] given classification classes such as [0, 1, 2] the class names, for example: ['high', 'medium', 'low'] required title Text the text to display at the top of the matrix. Defaults to 'Confusion matrix'. 'Confusion matrix' cmap matplotlib . colors . LinearSegmentedColormap the gradient of the values displayed from matplotlib.pyplot.cm see http://matplotlib.org/examples/color/colormaps_reference.html plt.get_cmap('jet') or plt.cm.Blues. Defaults to None. None normalize bool If False, plot the raw numbers. If True, plot the proportions. Defaults to True. True Returns: Type Description plt . figure plt.gfc (plt.figure): plotted confusion matrix Source code in src\\report\\visualize.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def plot_confusion_matrix ( cm : np . array , target_names : List [ Text ], title : Text = 'Confusion matrix' , cmap : matplotlib . colors . LinearSegmentedColormap = None , normalize : bool = True ) -> plt . figure : \"\"\"Given a sklearn confusion matrix (cm), make a plot. Usage: plot_confusion_matrix(cm = cm, normalize = True, target_names = y_labels_vals, title = best_estimator_name) Citation: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html Args: cm (np.array): confusion matrix from sklearn.metrics.confusion_matrix target_names (List[Text]): given classification classes such as [0, 1, 2] the class names, for example: ['high', 'medium', 'low'] title (Text, optional): the text to display at the top of the matrix. Defaults to 'Confusion matrix'. cmap (matplotlib.colors.LinearSegmentedColormap, optional): the gradient of the values displayed from matplotlib.pyplot.cm see http://matplotlib.org/examples/color/colormaps_reference.html plt.get_cmap('jet') or plt.cm.Blues. Defaults to None. normalize (bool, optional): If False, plot the raw numbers. If True, plot the proportions. Defaults to True. Returns: plt.gfc (plt.figure): plotted confusion matrix \"\"\" accuracy = np . trace ( cm ) / float ( np . sum ( cm )) misclass = 1 - accuracy if cmap is None : cmap = plt . get_cmap ( 'Blues' ) plt . figure ( figsize = ( 8 , 6 )) plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar () if target_names is not None : tick_marks = np . arange ( len ( target_names )) plt . xticks ( tick_marks , target_names , rotation = 45 ) plt . yticks ( tick_marks , target_names ) if normalize : cm = cm . astype ( 'float' ) / cm . sum ( axis = 1 )[:, np . newaxis ] thresh = cm . max () / 1.5 if normalize else cm . max () / 2 for i , j in itertools . product ( range ( cm . shape [ 0 ]), range ( cm . shape [ 1 ])): if normalize : plt . text ( j , i , \" {:0.4f} \" . format ( cm [ i , j ]), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) else : plt . text ( j , i , \" {:,} \" . format ( cm [ i , j ]), horizontalalignment = \"center\" , color = \"white\" if cm [ i , j ] > thresh else \"black\" ) plt . tight_layout () plt . ylabel ( 'True label' ) plt . xlabel ( 'Predicted label \\n accuracy= {:0.4f} ; misclass= {:0.4f} ' . format ( accuracy , misclass )) return plt . gcf ()","title":"plot_confusion_matrix()"},{"location":"api/#stages","text":"","title":"Stages"},{"location":"api/#data-load","text":"Module for the data loading stage.","title":"Data Load"},{"location":"api/#src.stages.data_load.data_load","text":"Load raw data. Depending on the parameter revision_on (taken from config), a specific version of data is loaded from the DVC repo. Othewise, the iris dataset is loaded from sklearn.datasets. Parameters: Name Type Description Default config_path Text path to config required Source code in src\\stages\\data_load.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @mlflow_run def data_load ( config_path : Text ) -> None : \"\"\"Load raw data. Depending on the parameter revision_on (taken from config), a specific version of data is loaded from the DVC repo. Othewise, the iris dataset is loaded from sklearn.datasets. Args: config_path (Text): path to config \"\"\" with open ( config_path ) as conf_file : config = yaml . safe_load ( conf_file ) logger = get_logger ( 'DATA_LOAD' , log_level = config [ 'base' ][ 'log_level' ]) logger . info ( 'Get dataset' ) if config [ 'data_load' ][ 'revision_on' ]: logger . info ( 'Grab dataset version {} ' . format ( config [ 'data_load' ][ 'version' ])) data_url = dvc . api . get_url ( path = config [ 'data_load' ][ 'dataset_csv' ], repo = config [ 'data_load' ][ 'repo' ], rev = config [ 'data_load' ][ 'version' ] ) dataset = pd . read_csv ( data_url , sep = ',' ) # log data url and version tag in mlflow mlflow . log_param ( 'data_url' , data_url ) mlflow . log_param ( 'version' , config [ 'data_load' ][ 'version' ]) else : data = load_iris ( as_frame = True ) dataset = data . frame dataset . rename ( columns = lambda colname : colname . strip ( ' (cm)' ) . replace ( ' ' , '_' ), inplace = True ) logger . info ( 'Save raw data' ) dataset . to_csv ( config [ 'data_load' ][ 'dataset_csv' ], index = False ) mlflow . log_param ( 'input_rows' , dataset . shape [ 0 ]) mlflow . log_param ( 'input_cols' , dataset . shape [ 1 ])","title":"data_load()"},{"location":"api/#data-split","text":"Module for the data split stage.","title":"Data Split"},{"location":"api/#src.stages.data_split.data_split","text":"Split the featurized dataset into train/test. Parameters: Name Type Description Default config_path Text path to config required Source code in src\\stages\\data_split.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @mlflow_run def data_split ( config_path : Text ) -> None : \"\"\"Split the featurized dataset into train/test. Args: config_path (Text): path to config \"\"\" with open ( config_path ) as conf_file : config = yaml . safe_load ( conf_file ) logger = get_logger ( 'DATA_SPLIT' , log_level = config [ 'base' ][ 'log_level' ]) logger . info ( 'Load features' ) dataset = pd . read_csv ( config [ 'featurize' ][ 'features_path' ]) logger . info ( 'Split features into train and test sets' ) train_dataset , test_dataset = train_test_split ( dataset , test_size = config [ 'data_split' ][ 'test_size' ], random_state = config [ 'base' ][ 'random_state' ] ) logger . info ( 'Save train and test sets' ) train_csv_path = config [ 'data_split' ][ 'trainset_path' ] test_csv_path = config [ 'data_split' ][ 'testset_path' ] train_dataset . to_csv ( train_csv_path , index = False ) test_dataset . to_csv ( test_csv_path , index = False ) mlflow . log_param ( 'test_size' , config [ 'featurize' ][ 'features_path' ])","title":"data_split()"},{"location":"api/#evaluation","text":"Module for evaluation stage of trained model.","title":"Evaluation"},{"location":"api/#src.stages.evaluate.evaluate_model","text":"Evaluate the trained model. Load the trained model and the test set via the path specification in the config file. Do the prediction on the test set, compute its f1 score and plot the confusion matrix. Both are saved in the reports folder. Parameters: Name Type Description Default config_path Text path to config required Source code in src\\stages\\evaluate.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 @mlflow_run def evaluate_model ( config_path : Text ) -> None : \"\"\"Evaluate the trained model. Load the trained model and the test set via the path specification in the config file. Do the prediction on the test set, compute its f1 score and plot the confusion matrix. Both are saved in the reports folder. Args: config_path (Text): path to config \"\"\" with open ( config_path ) as conf_file : config = yaml . safe_load ( conf_file ) logger = get_logger ( 'EVALUATE' , log_level = config [ 'base' ][ 'log_level' ]) logger . info ( 'Load model' ) model_path = config [ 'train' ][ 'model_path' ] model = joblib . load ( model_path ) logger . info ( 'Load test dataset' ) test_df = pd . read_csv ( config [ 'data_split' ][ 'testset_path' ]) logger . info ( 'Evaluate (build report)' ) target_column = config [ 'featurize' ][ 'target_column' ] y_test = test_df . loc [:, target_column ] . values X_test = test_df . drop ( target_column , axis = 1 ) . values prediction = model . predict ( X_test ) f1 = f1_score ( y_true = y_test , y_pred = prediction , average = 'macro' ) cm = confusion_matrix ( prediction , y_test ) report = { 'f1' : f1 , 'cm' : cm , 'actual' : y_test , 'predicted' : prediction } logger . info ( 'Save metrics' ) # save f1 metrics file reports_folder = Path ( config [ 'evaluate' ][ 'reports_dir' ]) metrics_path = reports_folder / config [ 'evaluate' ][ 'metrics_file' ] json . dump ( obj = { 'f1_score' : report [ 'f1' ]}, fp = open ( metrics_path , 'w' ) ) logger . info ( f 'F1 metrics file saved to : { metrics_path } ' ) logger . info ( 'Save confusion matrix' ) # save confusion_matrix.png plt = plot_confusion_matrix ( cm = report [ 'cm' ], target_names = load_iris ( as_frame = True ) . target_names . tolist (), normalize = False ) confusion_matrix_png_path = reports_folder / \\ config [ 'evaluate' ][ 'confusion_matrix_image' ] plt . savefig ( confusion_matrix_png_path ) logger . info ( f 'Confusion matrix saved to : { confusion_matrix_png_path } ' ) mlflow . log_metric ( 'f1_test' , report [ 'f1' ])","title":"evaluate_model()"},{"location":"api/#featurize","text":"Module for featurization stage of raw data sets.","title":"Featurize"},{"location":"api/#src.stages.featurize.featurize","text":"Create new features and save the processed dataset. Parameters: Name Type Description Default config_path Text path to config required Source code in src\\stages\\featurize.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @mlflow_run def featurize ( config_path : Text ) -> None : \"\"\"Create new features and save the processed dataset. Args: config_path (Text): path to config \"\"\" with open ( config_path ) as conf_file : config = yaml . safe_load ( conf_file ) logger = get_logger ( 'FEATURIZE' , log_level = config [ 'base' ][ 'log_level' ]) logger . info ( 'Load raw data' ) dataset = pd . read_csv ( config [ 'data_load' ][ 'dataset_csv' ]) logger . info ( 'Extract features' ) dataset [ 'sepal_length_to_sepal_width' ] = dataset [ 'sepal_length' ] / \\ dataset [ 'sepal_width' ] dataset [ 'petal_length_to_petal_width' ] = dataset [ 'petal_length' ] / \\ dataset [ 'petal_width' ] featured_dataset = dataset [[ 'sepal_length' , 'sepal_width' , 'petal_length' , 'petal_width' , 'sepal_length_to_sepal_width' , 'petal_length_to_petal_width' , 'target' ]] logger . info ( 'Save features' ) features_path = config [ 'featurize' ][ 'features_path' ] featured_dataset . to_csv ( features_path , index = False ) mlflow . log_param ( 'features' , featured_dataset . columns )","title":"featurize()"},{"location":"api/#train","text":"Module for the training stage.","title":"Train"},{"location":"api/#src.stages.train.train_model","text":"Train model. Train the model on the train set. Get the path to the data and the parametrization for the model used from the config. This stage uses a helper function that does a GridSearchCV, with the intend of further modularization and higher flexibility of this script. The best model is saved to the models directory as a joblib file. In addition, the model is logged via mlflow. Parameters: Name Type Description Default config_path Text path to config required Source code in src\\stages\\train.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 @mlflow_run def train_model ( config_path : Text ) -> None : \"\"\"Train model. Train the model on the train set. Get the path to the data and the parametrization for the model used from the config. This stage uses a helper function that does a GridSearchCV, with the intend of further modularization and higher flexibility of this script. The best model is saved to the models directory as a joblib file. In addition, the model is logged via mlflow. Args: config_path (Text): path to config \"\"\" with open ( config_path ) as conf_file : config = yaml . safe_load ( conf_file ) logger = get_logger ( 'TRAIN' , log_level = config [ 'base' ][ 'log_level' ]) logger . info ( 'Get estimator name' ) estimator_name = config [ 'train' ][ 'estimator_name' ] logger . info ( f 'Estimator: { estimator_name } ' ) logger . info ( 'Load train dataset' ) train_df = pd . read_csv ( config [ 'data_split' ][ 'trainset_path' ]) logger . info ( 'Train model' ) model = train ( df = train_df , target_column = config [ 'featurize' ][ 'target_column' ], estimator_name = estimator_name , param_grid = config [ 'train' ][ 'estimators' ][ estimator_name ][ 'param_grid' ], cv = config [ 'train' ][ 'cv' ] ) logger . info ( f 'Best score: { model . best_score_ } ' ) logger . info ( 'Save model' ) models_path = config [ 'train' ][ 'model_path' ] joblib . dump ( model , models_path ) mlflow . log_param ( 'chosen_estimator' , model . best_estimator_ ) mlflow . log_param ( 'parametrization' , model . best_params_ ) mlflow . log_metric ( 'F1' , model . best_score_ ) mlflow . sklearn . log_model ( model , 'model' )","title":"train_model()"},{"location":"api/#train-submodule","text":"Contains the submodule for training the model. Consider adding more options.","title":"Train Submodule"},{"location":"api/#src.train.train.get_supported_estimator","text":"Return a list of supported classifiers. Returns: Name Type Description Dict Dict supported classifiers Source code in src\\train\\train.py 20 21 22 23 24 25 26 27 28 29 30 def get_supported_estimator () -> Dict : \"\"\"Return a list of supported classifiers. Returns: Dict: supported classifiers \"\"\" return { 'logreg' : LogisticRegression , 'svm' : SVC , 'knn' : KNeighborsClassifier }","title":"get_supported_estimator()"},{"location":"api/#src.train.train.train","text":"Train model via GridSearchCV. First, check if the estimator handed to this function is supported by the GridSearchCV subroutine. If so, do the gridsearch and return the fitted object. Parameters: Name Type Description Default df pd . DataFrame dataset required target_column Text target column name required estimator_name Text estimator name required param_grid Dict grid parameters required cv int cross-validation value required Raises: Type Description UnsupportedClassifier Raise an error if estimator is not supported by this train routine. Returns: Name Type Description clf sklearn . grid_search trained model Source code in src\\train\\train.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def train ( df : pd . DataFrame , target_column : Text , estimator_name : Text , param_grid : Dict , cv : int ): \"\"\"Train model via GridSearchCV. First, check if the estimator handed to this function is supported by the GridSearchCV subroutine. If so, do the gridsearch and return the fitted object. Args: df (pd.DataFrame): dataset target_column (Text): target column name estimator_name (Text): estimator name param_grid (Dict): grid parameters cv (int): cross-validation value Raises: UnsupportedClassifier: Raise an error if estimator is not supported by this train routine. Returns: clf (sklearn.grid_search): trained model \"\"\" estimators = get_supported_estimator () if estimator_name not in estimators . keys (): raise UnsupportedClassifier ( estimator_name ) estimator = estimators [ estimator_name ]() f1_scorer = make_scorer ( f1_score , average = 'weighted' ) clf = GridSearchCV ( estimator = estimator , param_grid = param_grid , cv = cv , verbose = 1 , scoring = f1_scorer ) # Get X and Y y_train = df . loc [:, target_column ] . values . astype ( 'int32' ) X_train = df . drop ( target_column , axis = 1 ) . values . astype ( 'float32' ) clf . fit ( X_train , y_train ) return clf","title":"train()"},{"location":"api/#utils","text":"","title":"Utils"},{"location":"api/#logging","text":"Provides functions to create loggers.","title":"Logging"},{"location":"api/#src.utils.logs.get_console_handler","text":"Get console handler. Returns: Type Description logging . StreamHandler logging.StreamHandler: log into stdout Source code in src\\utils\\logs.py 7 8 9 10 11 12 13 14 15 16 17 18 def get_console_handler () -> logging . StreamHandler : \"\"\"Get console handler. Returns: logging.StreamHandler: log into stdout \"\"\" console_handler = logging . StreamHandler ( sys . stdout ) formatter = logging . Formatter ( \" %(asctime)s \u2014 %(name)s \u2014 %(levelname)s \u2014 %(message)s \" ) console_handler . setFormatter ( formatter ) return console_handler","title":"get_console_handler()"},{"location":"api/#src.utils.logs.get_logger","text":"Get logger. Parameters: Name Type Description Default name Text logger name. Defaults to name . __name__ log_level Union [ Text , int ] logging level; can be string name or integer value. Defaults to logging.DEBUG. logging.DEBUG Returns: Type Description logging . Logger logging.Logger: logger instance Source code in src\\utils\\logs.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def get_logger ( name : Text = __name__ , log_level : Union [ Text , int ] = logging . DEBUG ) -> logging . Logger : \"\"\"Get logger. Args: name (Text, optional): logger name. Defaults to __name__. log_level (Union[Text, int], optional): logging level; can be string name or integer value. Defaults to logging.DEBUG. Returns: logging.Logger: logger instance \"\"\" logger = logging . getLogger ( name ) logger . setLevel ( log_level ) # Prevent duplicate outputs in Jypyter Notebook if logger . hasHandlers (): logger . handlers . clear () logger . addHandler ( get_console_handler ()) logger . propagate = False return logger","title":"get_logger()"},{"location":"api/#mlflow-decorator","text":"Module for creating the MLFlow nested run decorator.","title":"MLFlow Decorator"},{"location":"api/#src.utils.mlflow_run_decorator.mlflow_run","text":"Decorator to turn every stage into MLflow nested run. Parameters: Name Type Description Default wrapped_function Callable the respective function in each stage required Returns: Type Description Callable wrapped function (Callable): the input function, tracked under the respective MLFLOW_RUN_ID Source code in src\\utils\\mlflow_run_decorator.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def mlflow_run ( wrapped_function : Callable ) -> Callable : \"\"\"Decorator to turn every stage into MLflow nested run. Args: wrapped_function (Callable): the respective function in each stage Returns: wrapped function (Callable): the input function, tracked under the respective MLFLOW_RUN_ID \"\"\" config_path = 'params.yaml' with open ( config_path ) as conf_file : config = yaml . safe_load ( conf_file ) @wraps ( wrapped_function ) def wrapper ( * args , ** kwargs ): mlflow . set_experiment ( config [ 'base' ][ 'project_experiment_name' ]) # recover parent run thanks to MLFLOW_RUN_ID env variable with mlflow . start_run (): # start child run with mlflow . start_run ( run_name = wrapped_function . __name__ , nested = True ): return wrapped_function ( * args , ** kwargs ) return wrapper","title":"mlflow_run()"},{"location":"api/#initializing-the-nested-mlflow-run","text":"Module for initializing nested MLFlow run.","title":"Initializing the nested MLFlow run"},{"location":"api/#src.utils.start_pipeline.start_pipeline","text":"Start a new MLflow run each time we launch the DVC pipeline. Print the MLFLOW_RUN_ID and save as an environment variable. This allows the possibility to track nested runs for each stage of the pipeline. Parameters: Name Type Description Default config_path Text path to config required run_name str specify run name required Source code in src\\utils\\start_pipeline.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def start_pipeline ( config_path : Text , run_name : str ) -> None : \"\"\"Start a new MLflow run each time we launch the DVC pipeline. Print the MLFLOW_RUN_ID and save as an environment variable. This allows the possibility to track nested runs for each stage of the pipeline. Args: config_path (Text): path to config run_name (str): specify run name \"\"\" with open ( config_path ) as conf_file : config = yaml . safe_load ( conf_file ) mlflow . set_experiment ( config [ 'base' ][ 'project_experiment_name' ]) with mlflow . start_run ( run_name = run_name ): print ( mlflow . active_run () . info . run_id ) mlflow . log_artifact ( 'dvc.yaml' ) mlflow . log_artifact ( config_path )","title":"start_pipeline()"}]}